---
title: "STAT 3675 Final Project: Predicting People at Risk for Heart Attacks"
author: "Pavan Adapa"
date: "4/21/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(cowplot)
library(boot)
library(rpart)
library(rpart.plot)
library(party)
library(randomForest)
mydata = read.csv("heart.csv")
mydata$sex = factor(mydata$sex, ordered = FALSE)
mydata$cp = factor(mydata$cp, ordered = TRUE)
mydata$fbs = factor(mydata$fbs, ordered = FALSE)
mydata$restecg = factor(mydata$restecg, ordered = TRUE)
mydata$exng = factor(mydata$exng, ordered = FALSE)
mydata$slp = factor(mydata$slp, ordered = TRUE)
mydata$thall = factor(mydata$thall, ordered = TRUE)
mydata$output = factor(mydata$output, ordered = FALSE)
mydatatransformed = mydata
mydatatransformed$trtbps = log(mydatatransformed$trtbps)
set.seed(1)
randomizeddata<- mydata[sample(nrow(mydata)),]
trainingdata = randomizeddata[1:240,]
testingdata = randomizeddata[241:303,]
randomizeddatatransformed<- mydatatransformed[sample(nrow(mydata)),]
trainingdatatrans = randomizeddata[1:240,]
testingdatatrans = randomizeddata[241:303,]
```


## Introduction
Healthcare is not only reactionary and it also entails preventative health. Preventative health is necessary to ensure longer lifespans and higher-quality lives. Millions of people die from preventable causes every year and increasing preventative care can end more needless tragedies. Classifications can be a very powerful tool for diagnosis as they can aid doctors where their biases might fail them otherwise. This project will attempt to predict people at risk for heart attacks by using several different classification methods and then pick the best model. The methods used will be logistic regression, classical decision trees, conditional inference trees, and random forest. The dataset was acquired from the University of Irvine Machine Learning Repository.


## Elementary Data Anaylsis
The dataset has 303 observations and of that data, approximately 80% will be the training data (240 observations). Therefore the remaining data (~20% or 63 observations) will the testing dataset. The dataset has 14 variables, and thus there are 13 independent variables. There are 7 categorical variables in the dataset of which two ordinal variables. The independent variables are: <br />
1. Age: age in years <br />   
2. Sex: sex (1 = male; 0 = female) <br />   
3. Cp: chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 4 = asymptomatic) <br /> 
4. Trestbps: resting blood pressure (in mm Hg on admission to the hospital) <br />   
5. Chol: serum cholesterol in mg/dl <br />   
6. Fbs: fasting blood sugar > 120 mg/dl (1 = true; 0 = false) <br />   
7. Restecg: resting electrocardiographic results (0 = normal; 1 = having ST-T; 2 = hypertrophy) <br />
8. Thalach: maximum heart rate achieved <br />   
9. Exang: exercise induced angina (1 = yes; 0 = no) <br />   
10. Oldpeak: ST depression induced by exercise relative to rest <br />   
11. Slope: the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping) <br />  
12. Ca: number of major vessels (0-3) colored by fluoroscopy <br />   
13. Thal: Thalassemia (normal = 1; fixed defect = 2; reversible defect = 3) <br />
The independent variables is: <br />
Output: diagnosis of a heart attack (1 = yes, 0 = no) <br />   
Below is the summary of the data. <br />   
```{r, echo = FALSE }
summary(mydata)
```

**1. Graphs Exploring Data**

```{r, fig.width=6.4, fig.height=3, echo = FALSE }
ggplot(mydata, aes(output, fill = factor(output)))+
        geom_bar(show.legend = FALSE)+
        ggtitle("Barplot of the Heart Attack Outcomes")+
        geom_text(aes(label = ..count..), stat = "count", vjust = 1.5, colour = "white")+
        xlab("Heart Attack Outcome")+
        labs(fill='NEW LEGEND TITLE')+
        scale_x_discrete(labels=c("0" = "Negative", "1" = "Positive"))+
        theme_minimal()+ 
        theme(plot.title = element_text(hjust = 0.5))
```
Interestingly the data has more occurrences of people having a heart attack than not.

```{r, fig.width=6.4, fig.height=3.5, echo = FALSE }
p2 = ggplot(mydata, aes(y = age))+
        geom_boxplot()+
        ylab("age (years)")+
        theme_minimal()+ 
        theme(plot.title = element_text(hjust = 0.5))+
        theme(axis.title.x=element_blank(), axis.text.x=element_blank(),
              axis.ticks.x=element_blank())
p3 = ggplot(mydatatransformed, aes(y = trtbps))+
        geom_boxplot()+
        theme_minimal()+ 
        ylab("blood pressure (mm Hg)")+
        theme(plot.title = element_text(hjust = 0.5)) +
        theme(axis.title.x=element_blank(), axis.text.x=element_blank(),
              axis.ticks.x=element_blank())
p4 = ggplot(mydata, aes(y = chol))+
        geom_boxplot()+
        theme_minimal()+ 
        ylab("cholestrol (mg/dl)" )+
        theme(plot.title = element_text(hjust = 0.5))+
        theme(axis.title.x=element_blank(), axis.text.x=element_blank(),
              axis.ticks.x=element_blank())
  
p5 = ggplot(mydata, aes(y = thalachh))+
        geom_boxplot()+
        theme_minimal()+ 
        ylab("heart rate (BPM)" )+
        theme(plot.title = element_text(hjust = 0.5))+
        theme(axis.title.x=element_blank(), axis.text.x=element_blank(),
              axis.ticks.x=element_blank())
p6 = ggplot(mydata, aes(y = oldpeak))+
        geom_boxplot()+
        theme_minimal()+ 
        ylab("peak" )+
        theme(plot.title = element_text(hjust = 0.5))+
        theme(axis.title.x=element_blank(), axis.text.x=element_blank(),
              axis.ticks.x=element_blank())
p = plot_grid(p2, p3, p4, p5, p6)
title <- ggdraw() + draw_label("Distributions of Quantative Variables", fontface='bold')
plot_grid(title, p, ncol=1, rel_heights=c(0.2, 1))
```

While there are some outliers in the some of the distributions, none of the outliers have to be removed as most classification methods, including the ones used in this paper, doesn't require parametric distributions and thus are robust to outliers. 

## Logistic Regression

The first method of classification will be the logistic Regression. 
Below is the the original logistic model where no coefficients are dropped:
```{r, echo = FALSE}
formulaf = as.formula(paste('factor(output) ~', paste(colnames(trainingdata)[1:13], collapse='+')))
ha.lr = glm(formulaf, data= trainingdata, family = binomial)
ha.lr
```

In the next few sections, all types of stepwise regressions will be performed to find the best model. <br />  

**1. Backward Elimination**
```{r, echo = FALSE}
ha.lrback = step(ha.lr, trace = 0)
ha.lrback
```

Backward elimination got rid of 5 variables. The variables removed where age, chol, fbs, restingecg, exng. Interestingly this model doesn't consider age as an important predictor for heart attacks. However, the AIC dropped form 205.3 to 196.5. 

**2. Forward Selection**
```{r, echo = FALSE}
ha.lrforward = step(ha.lr, direction = "forward", trace = 0)
ha.lrforward
```

Forward selection did not get rid of any variables and thus it the same as the orginal model.

**3. Bidirectional Elimination**
```{r, echo = FALSE}
ha.lrboth = step(ha.lr, direction = "both", trace = 0)
ha.lrboth
```

Bidirectional elimination resulted in the same model as the backward elimination model.

**4. The Best Logistic Model**


The two logistic models will be assessed against the remaining 25% data, the training data, to see which model has more predictive power.

A. Backward Elimination/ Bidirectional Elimination
```{r, echo = FALSE}
performance <- function(table, n=2){
if(!all(dim(table) == c(2,2)))
stop("Must be a 2 x 2 table")
tn = table[1,1]
fp = table[1,2]
fn = table[2,1]
tp = table[2,2]
hitrate = (tp+tn)/(tp+tn+fp+fn)
result <- paste("Accuracy = ", round(hitrate, n), "\n", sep="")
cat(result)
}
prob <- predict(ha.lrback, testingdata, type="response")
logit.pred <- factor(prob > .5, levels=c(FALSE, TRUE),
labels=c("0", "1"))
logit.perf <- table(testingdata$output, logit.pred,
dnn=c("Actual", "Predicted"))
logit.perf
performance(logit.perf)
```

B. Orginal/ Forward Selection
```{r, echo = FALSE}
prob <- predict(ha.lr, testingdata, type="response")
logit.pred <- factor(prob > .5, levels=c(FALSE, TRUE),
labels=c("0", "1"))
logit.perf <- table(testingdata$output, logit.pred,
dnn=c("Actual", "Predicted"))
logit.perf
performance(logit.perf)
```

This is a tough choice as both regression models excel at different areas. While the backward elimination regression model has a slightly lower AIC, it had slightly lower predictive power. The backward elimination model selected the wrong classification 11 times or 17% percent of the time compared to the original model which elected the wrong classification 10 times. As the differences in AIC and predictive power where almost negliblle, the original model was kept.

## Classical Decision Trees 

**1. Complete Tree** \
Below is a graphical representation of the Complete decision tree and CP (Complexity parameter) table and plot. \
```{r, echo = FALSE}
set.seed(1234)
hadtree <- rpart(formulaf, data= trainingdata, method="class", parms=list(split="information"))
prp(hadtree, type = 2, extra = 104,
fallen.leaves = TRUE, main="Complete Decision Tree")
hadtree$cptable
```
```{r, fig.height = 3.3, fig.align = "center", echo = FALSE}
plotcp(hadtree)
```

**2. Pruned Tree**

Based from the CP table and plot, the decision tree will be pruned at the 3 nsplit or where the CP = .027. Below is the graph of the pruned decision tree and its classification of the testing data.
```{r, echo = FALSE}
hadtree.pruned <- prune(hadtree, cp=.027)
prp(hadtree.pruned, type = 2, extra = 104,
fallen.leaves = TRUE, main=" Pruned Decision Tree")
dtree.pred <- predict(hadtree.pruned, testingdata, type="class")
dtree.perf <- table(testingdata$output, dtree.pred,
dnn=c("Actual", "Predicted"))
performance(dtree.perf)
```

## Conditional Inference Trees

Below is the graphical representation of Conditional Inference Tree and its classification of the testing data.
```{r, echo = FALSE}
ha.ctree <- ctree(formulaf, data= mydata)
plot(ha.ctree, main="Conditional Inference Tree")
ctree.pred <- predict(ha.ctree, testingdata, type="response")
ctree.perf <- table(testingdata$output, ctree.pred,
dnn=c("Actual", "Predicted"))
ctree.perf
```

## Random Forest

Below is the code output of Random Forest and its classification of the testing data.
```{r, echo = FALSE}
set.seed(1234)
fit.forest <- randomForest(formulaf, data= trainingdata, na.action=na.roughfix, importance=TRUE)
fit.forest
importance(fit.forest, type=2)
forest.pred <- predict(fit.forest, testingdata)
forest.perf <- table(testingdata$output, forest.pred,
dnn=c("Actual", "Predicted"))
forest.perf
```

## Assesing Classification Accuracy 
```{r, echo = FALSE}
performance <- function(table, n=2){
if(!all(dim(table) == c(2,2)))
stop("Must be a 2 x 2 table")
tn = table[1,1]
fp = table[1,2]
fn = table[2,1]
tp = table[2,2]
sensitivity = tp/(tp+fn)
specificity = tn/(tn+fp)
ppp = tp/(tp+fp)
npp = tn/(tn+fn)
hitrate = (tp+tn)/(tp+tn+fp+fn)
result <- paste("Sensitivity = ", round(sensitivity, n) ,
"\nSpecificity = ", round(specificity, n),
"\nPositive Predictive Value = ", round(ppp, n),
"\nNegative Predictive Value = ", round(npp, n),
"\nAccuracy = ", round(hitrate, n), "\n", sep="")
cat(result)
}
```

1. Performance of the Logistic Regression
```{r, echo = FALSE}
performance(logit.perf)
```

2. Performance of the Classical Decision Tree
```{r, echo = FALSE}
performance(dtree.perf)
```

3. Performance of the Conditional Inference Tree
```{r, echo = FALSE}
performance(ctree.perf)
```

4. Performance of the Random Forest
```{r, echo = FALSE}
performance(forest.perf)
```

## Conclusion 
While all models had a relatively high predictive power, with each model having at least an 80% accuracy. The Random Forest had the most accuracy, with it being accurate almost 90% of the time. However, it is important to note that while it is the most accurate it is the least interpretable of the models. If a more interpretable model was necessary a conditional inference tree could be selected instead. And this could make sense in a medical setting as patients might be worried about how the model diagnosed them. <br />

## References
Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D. \
University Hospital, Zurich, Switzerland: William Steinbrunn, M.D. \
University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D. \
V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:Robert Detrano, M.D., Ph.D. \




